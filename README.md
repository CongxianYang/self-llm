
# ollamaéƒ¨ç½²--å¿«é€Ÿå¼€å§‹

### å¸¸ç”¨å‘½ä»¤
```
ollama run llama3   # æ‹‰å–llama3æ¨¡å‹å¹¶æ‰§è¡Œ
ollama list         # æ˜¾ç¤ºæ¨¡å‹åˆ—è¡¨
ollama rm llama3    # åˆ é™¤llama3æ¨¡å‹
ollama pull         # æ‹‰å–ollamaçº¿ä¸Šå­˜åœ¨çš„æ¨¡å‹ï¼Œå¹¶ä¸æ‰§è¡Œ

```
### 1.åˆ›å»ºä½¿ç”¨æœ¬åœ°model

1.1 æ„å»º Modelfile æ–‡ä»¶
```
FROM PATH_TO_YOUR__GGUF_MODEL

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM """
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
"""
```

1.2 åŠ è½½æ¨¡å‹ä» Modelfile æ–‡ä»¶
```
  ollama create NAME -f ./Modelfile

# NAME: åœ¨ollamaä¸­æ˜¾ç¤ºçš„åç§°
# ./Modelfile: ç»å¯¹æˆ–è€…ç›¸å¯¹è·¯å¾„
```
## tips: ollamaå¹¶ä¸æ”¯æŒæ‰€æœ‰ä»llama.cppä¸­å¯¼å‡ºçš„æ¨¡å‹S
### 2. å¯åŠ¨æœåŠ¡
```
ollama server  
```

### 3. ä½¿ç”¨å¯¹è¯æ¨¡å¼

3.1 æµå¼è°ƒç”¨
```
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Why is the sky blue?"
}'
```

3.2 éæµå¼è°ƒç”¨
```
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```

3.3 å¸¦ä¸Šä¸‹æ–‡è°ƒç”¨ï¼ˆå¤šè½®å¯¹è¯ï¼‰
```
curl http://localhost:11434/api/chat -d '{
  "model": "llama3",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    },
    {
      "role": "assistant",
      "content": "due to rayleigh scattering."
    },
    {
      "role": "user",
      "content": "how is that different than mie scattering?"
    }
  ],
  "stream": false
}'

```

# vllméƒ¨ç½²å¤§æ¨¡å‹--å¿«é€Ÿå¼€å§‹--ä¸éœ€è¦é‡åŒ–

### 1.ä½¿ç”¨æœ¬åœ°model
```
  your modelpath
```
### 2. å®‰è£…vllm
```
pip install vllm 
```

### 3. ä½¿ç”¨vllm

3.1 æ¨ç†æµ‹è¯•
```
python infer_test.py
```

3.2 éƒ¨ç½²æœåŠ¡
```
bash run_vllm_server.sh
```

3.3 è°ƒç”¨æœåŠ¡
```
# 1.ä½¿ç”¨openaié£æ ¼çš„å®¢æˆ·ç«¯è°ƒç”¨
python request.py

# 2. ä½¿ç”¨gradioå®¢æˆ·ç«¯
bash run_gradio_client.sh

```


# LLaMA-Factoryå¾®è°ƒå¤§æ¨¡å‹ğŸ¦•
1.å®‰è£…é¡¹ç›®çš„ä¾èµ–åŒ…
```bash
pip install -e .
```
2.å¿«é€Ÿå¾®è°ƒï¼Œæ¨ç†ï¼Œå¯¼å‡ºæ¨¡å‹ï¼Œä»¥Llama3ä¸ºä¾‹å­<br><br>
TipğŸŒ¼:ä¸ç¡®å®šçš„ï¼Œå¯¹äºæ‰€æœ‰â€œåŸºåº§â€ï¼ˆBaseï¼‰æ¨¡å‹ï¼Œtemplate å‚æ•°å¯ä»¥æ˜¯ default, alpaca, vicuna ç­‰ä»»æ„å€¼ã€‚<br>
ä½†â€œå¯¹è¯â€ï¼ˆInstruct/Chatï¼‰æ¨¡å‹è¯·åŠ¡å¿…ä½¿ç”¨å¯¹åº”çš„æ¨¡æ¿ã€‚è¯·åŠ¡å¿…åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é‡‡ç”¨å®Œå…¨ä¸€è‡´çš„æ¨¡æ¿ã€‚
```bash
llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
```
å¯åŠ¨å¯è§†åŒ–ç»ˆç«¯å¾®è°ƒ
```bash
cd LLaMA-Factory
llamafactory-cli webui
```
# llama.cppé‡åŒ–éƒ¨ç½²å¤§æ¨¡å‹ğŸ¦•
1.ç¼–è¯‘CPPæ–‡ä»¶
åŸºäºCUDA
```bash
cd llama.cpp
cmake -B build_cuda -DLLAMA_CUDA=ON

cmake --build build_cuda --config Release -j 12
```

2.è½¬åŒ–safetensorsæ¨¡å‹æ ¼å¼ä¸ºggufå¹¶é‡åŒ–ä¸ºQ8_0
```bash
python convert-hf-to-gguf.py  /modelspath/xxx --outfile  /outmodel/modelname-q8_0-v1.gguf --outtype q8_0
```
3.ç»ˆç«¯è¿è¡Œggufæ ¼å¼æ¨¡å‹
```bash
cd llama.cpp/build_cuda/bin/
./llama-cli -m /modelpath/xxxx.gguf -cnv
```
4.å¼€å¯æœåŠ¡å™¨
```bash
./llama-server -m our_model.gguf --port 8080

# Basic web UI can be accessed via browser: http://localhost:8080
```
5.è¿æ¥å¼€å¯çš„æœåŠ¡å™¨
```bash
python request.py
```
6.å¯¹ggufæ ¼å¼Q8_0æ¨¡å‹å†é‡åŒ–ä¸ºQ4_1

```bash
cd llama.cpp/build_cuda/bin
./quantize --allow-requantize /modelspath/xxx-q8_0-v2_1.gguf /outmodelspath/xxx-q4_1-v1.gguf Q4_1
