# LLaMA-Factoryå¾®è°ƒğŸ¦•
1.å®‰è£…é¡¹ç›®çš„ä¾èµ–åŒ…
```bash
pip install -e .
```
2.å¿«é€Ÿå¾®è°ƒï¼Œæ¨ç†ï¼Œå¯¼å‡ºæ¨¡å‹ï¼Œä»¥Llama3ä¸ºä¾‹å­<br><br>
TipğŸŒ¼:ä¸ç¡®å®šçš„ï¼Œå¯¹äºæ‰€æœ‰â€œåŸºåº§â€ï¼ˆBaseï¼‰æ¨¡å‹ï¼Œtemplate å‚æ•°å¯ä»¥æ˜¯ default, alpaca, vicuna ç­‰ä»»æ„å€¼ã€‚<br>
ä½†â€œå¯¹è¯â€ï¼ˆInstruct/Chatï¼‰æ¨¡å‹è¯·åŠ¡å¿…ä½¿ç”¨å¯¹åº”çš„æ¨¡æ¿ã€‚è¯·åŠ¡å¿…åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é‡‡ç”¨å®Œå…¨ä¸€è‡´çš„æ¨¡æ¿ã€‚
```bash
llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
```
å¯åŠ¨å¯è§†åŒ–ç»ˆç«¯å¾®è°ƒ
```bash
cd LLaMA-Factory
llamafactory-cli webui
```
# llama.cppé‡åŒ–éƒ¨ç½²æ¨¡å‹ğŸ¦•
1.ç¼–è¯‘CPPæ–‡ä»¶
åŸºäºCUDA
```bash
cd llama.cpp
cmake -B build_cuda -DLLAMA_CUDA=ON
cmake --build build_cuda --config Release -j 12
```

2.è½¬åŒ–safetensorsæ¨¡å‹æ ¼å¼ä¸ºggufå¹¶é‡åŒ–ä¸ºQ8_0
```bash
python convert_hf_to_gguf.py  /modelspath/xxx --outfile  /outmodel/modelname-q8_0-v1.gguf --outtype q8_0
```
3.ç»ˆç«¯è¿è¡Œggufæ ¼å¼æ¨¡å‹
```bash
cd llama.cpp/build_cuda/bin/
./llama-cli -m modelpath/xxxx.gguf -cnv
./llama-cli -m modelpath/xxxx.gguf -p "You are a helpful assistant" -cnv --in-prefix 'User: ' --reverse-prompt 'User:'
```
4.å¼€å¯æœåŠ¡å™¨
```bash
./llama-server -m our_model.gguf --port 8080

# Basic web UI can be accessed via browser: http://localhost:8080
```
5.ç”¨è„šæœ¬è¿æ¥æœåŠ¡å™¨
```bash
cd tools
python request1.py
```
6.å¯¹ggufæ ¼å¼Q8_0æ¨¡å‹å†é‡åŒ–ä¸ºQ4_1

```bash
cd llama.cpp/build_cuda/bin
./quantize --allow-requantize /modelspath/xxx-q8_0-v2_1.gguf /outmodelspath/xxx-q4_1-v1.gguf Q4_1
